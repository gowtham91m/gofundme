{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scraper.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/gowtham91m/gofundme/blob/master/Scraper.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "vbaSOpKwj4uK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install \"requests[security]\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oPt84Wphrm3y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "from IPython.display import display, clear_output\n",
        "import re\n",
        "import pandas as pd\n",
        "from time import time\n",
        "from collections import defaultdict\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u32P_SRW_TdU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class web_scraper:\n",
        "  def __init__(self,url):\n",
        "    self.url = url\n",
        "    \n",
        "  def get_categories(self):\n",
        "    soup = requests.get(url)\n",
        "    soup = bs(soup.text,'html.parser')\n",
        "    category = soup.findAll(class_='text-black')\n",
        "    categories = [i.text for i in category]  \n",
        "    return categories[:1] #16\n",
        "  \n",
        "  def details_parser(self,url):\n",
        "    \n",
        "    soup=bs(requests.get(url).text,'html.parser')\n",
        "    \n",
        "    try: text = soup.findAll(class_=\"co-story truncate-text truncate-text--description js-truncate\")[0].text.strip()\n",
        "    except: text = 'exception occured for' + url\n",
        "      \n",
        "    try: likes =  soup.findAll(class_='roundedNum')[0].text\n",
        "    except IndexError: likes = 0\n",
        "      \n",
        "    try: photos = soup.findAll(class_='open-media-viewer')[0].text.strip()\n",
        "    except IndexError: photos = 0\n",
        "    \n",
        "    try: shares = soup.findAll(class_='js-share-count-text')[0].text.strip()\n",
        "    except IndexError: shares = 0\n",
        "      \n",
        "    try: \n",
        "      donation = soup.findAll(class_='campaign-status text-small')[0].text.strip()\n",
        "      donation = donation.split(' ')\n",
        "      donation_count = donation[2]\n",
        "      duration = ' '.join(donation[-2:])\n",
        "    except:\n",
        "      donation_count = duration = 0\n",
        "    \n",
        "    return({'text':text, 'likes':likes, 'photos':photos, 'shares':shares, 'donation_count':donation_count, 'duration':duration})\n",
        "  \n",
        "  def scrape(self):\n",
        "    start_time = time()\n",
        "    df = pd.DataFrame({})\n",
        "    for i in self.get_categories():\n",
        "      print(i,end='  ')\n",
        "      i='-'.join(i.split(' '))\n",
        "      i = 'animal' if i == 'Animals' else i\n",
        "      url = 'https://www.gofundme.com/discover/'+i+'-fundraiser'\n",
        "      soup = bs(requests.get(url).text,'html.parser')\n",
        "      cid = re.findall('\\d+',re.findall('cid=\\'\\s\\+\\s\\'\\d+', soup.find_all('script')[13].text)[0])[0]\n",
        "      Resp,page = True,1\n",
        "      while Resp:\n",
        "        url = 'https://www.gofundme.com/mvc.php?route=categorypages/load_more&page='+str(page)+'&term=&cid='+cid\n",
        "        soup = requests.get(url)\n",
        "        soup = bs(soup.text, 'html.parser')\n",
        "        if len(soup) <1: Resp =False\n",
        "        name = [ hit.text  for hit in soup.findAll(attrs={'class' : 'fund-title truncate-single-line show-for-medium'})]\n",
        "        href = [i['href'] for i in soup.findAll('a',attrs={'class':'campaign-tile-img--contain'})]\n",
        "        location = [i.text[1:-1] for i in soup.findAll(class_='fund-item fund-location truncate-single-line')]\n",
        "        raised = [i.findAll('strong')[0].text[:-7] for i in soup.findAll(class_=\"fund-item truncate-single-line\")]\n",
        "        goal = [re.findall('\\$\\d+\\,\\d+',i.text)[1] for i in soup.findAll(class_=\"fund-item truncate-single-line\")]\n",
        "        \n",
        "        details =defaultdict(list)\n",
        "        for link in soup.findAll('a',attrs={'class':'campaign-tile-img--contain'}):\n",
        "          for key, value in self.details_parser(link['href']).items():\n",
        "            details[key].append(value)\n",
        "        \n",
        "        df = df.append(pd.DataFrame({'category':[i]*len(name),\n",
        "                                     'name':name,\n",
        "                                     'href':href,\n",
        "                                     'location':location, \n",
        "                                     'goal':goal,\n",
        "                                     'raised':raised,\n",
        "                                     'text':details['text'],\n",
        "                                     'likes':details['likes'],\n",
        "                                     'shares':details['shares'],\n",
        "                                     'photos':details['photos'],\n",
        "                                     'donation_count':details['donation_count'],\n",
        "                                     'duration':details['duration']}))\n",
        "        if (page%10==0):print(page,end=' ')\n",
        "        page+=1\n",
        "      print('\\n')\n",
        "    clear_output()\n",
        "    columns = ['category','name','href','location','goal','raised','text','likes','shares','photos','donation_count','duration']\n",
        "    print('campaigns scrape time', time()-start_time)\n",
        "    return df[columns]\n",
        "  \n",
        "  def get_donation_amount(self,df):\n",
        "    start_time = time()\n",
        "    donation_data = pd.DataFrame({})\n",
        "    for i in df.href:\n",
        "      campaign = i[25:]\n",
        "      url = 'https://www.gofundme.com/mvc.php?route=donate/pagingDonationsFoundation&url='+campaign+'&idx=10&type=recent'\n",
        "      soup = requests.get(url)\n",
        "      soup=bs(soup.text,'html.parser')\n",
        "      donation = [i.text for i in soup.findAll(class_='supporter-amount')]\n",
        "      time_gap = [i.text[:-4] for i in soup.findAll(class_='supporter-time')]\n",
        "      donation_data = donation_data.append(pd.DataFrame({'href':[i]*len(donation),\n",
        "                                                         'donation_amount':donation,\n",
        "                                                         'time':time_gap}))\n",
        "    print('donation amount scrape time', time()-start_time)\n",
        "    return donation_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SJJKVfxSGPIZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4d121b07-31c9-4ec1-d735-a756a6d40932"
      },
      "cell_type": "code",
      "source": [
        "url = 'https://www.gofundme.com/discover'\n",
        "scraper = web_scraper(url)\n",
        "campaign_data = scraper.scrape()\n",
        "#donation_data = scraper.get_donation_amount(campaign_data)\n",
        "# Merge campaign_data and donation_data on name"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "campaigns scrape time 590.7886605262756\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jmcUxwBG-IU0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#campaign_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kfR5EzFrGeiq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#donation_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kPh3EP35kIlC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "donation_data.to_csv('donation_data.csv',index=False)\n",
        "files.download('donation_data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}