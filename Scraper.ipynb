{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scraper.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/gowtham91m/gofundme/blob/master/Scraper.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "vbaSOpKwj4uK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install \"requests[security]\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oPt84Wphrm3y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "from IPython.display import display, clear_output\n",
        "import re\n",
        "import pandas as pd\n",
        "from time import time\n",
        "from collections import defaultdict\n",
        "from google.colab import files\n",
        "from collections import OrderedDict\n",
        "from datetime import datetime\n",
        "from itertools import accumulate\n",
        "import os\n",
        "import logging\n",
        "import subprocess"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u32P_SRW_TdU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class web_scraper:\n",
        "    def __init__(self):\n",
        "        self.url = 'https://www.gofundme.com/discover'\n",
        "        self.campaign_columns = ['category','page','title','href','location','start_date','goal','raised',\n",
        "                                 'text','likes','shares','photos','donation_count','duration',\n",
        "                                 'recent_donation_time','goal_reached_time','script_run_time']\n",
        "\n",
        "    def read_data(self):\n",
        "      #os.makedirs('gofundme/data',exist_ok=True)\n",
        "      os.chdir('/content/gofundme/data')\n",
        "      if 'campaigns.csv' not in os.listdir():\n",
        "            campaign_data = pd.DataFrame(OrderedDict({i:[] for i in self.campaign_columns}))\n",
        "            campaign_data.to_csv('campaigns.csv',index=False)\n",
        "      else:\n",
        "            campaign_data = pd.read_csv('campaigns.csv')\n",
        "      return campaign_data\n",
        "      \n",
        "    def git_clone(self):\n",
        "      os.chdir('/content')\n",
        "      if 'gofindme' not in os.listdir():\n",
        "        gt = 'https://@github.com/gowtham91m/gofundme.git'\n",
        "        subprocess.Popen(['git', 'clone', str(gt)])\n",
        "      else:\n",
        "        os.chdir('/content/gofundme')\n",
        "        subprocess.Popen(['git','pull'])\n",
        "        \n",
        "    def git_push(self):\n",
        "      os.chdir('/content/gofundme')\n",
        "      \n",
        "      !git config --global user.email \"gowtham.91m@gmail.com\"\n",
        "      !git config --global user.name \"Gowtham Mallikarjuna\"\n",
        "      !git add .\n",
        "      !git commit -m \"commit\"\n",
        "      !git push -u origin \"master\"\n",
        "\n",
        "      #subprocess.Popen(['git','add','.'])\n",
        "      #subprocess.Popen(['git','commit','-m','commit'])\n",
        "      #ubprocess.Popen(['git','push','-u','origin','master'])\n",
        "        \n",
        "    def get_categories(self):\n",
        "        soup = requests.get(self.url)\n",
        "        soup = bs(soup.text,'html.parser')\n",
        "        category = soup.findAll(class_='text-black')\n",
        "        categories = [i.text for i in category]  \n",
        "        return categories[:16]\n",
        "\n",
        "    def details_parser(self,url,category,page):\n",
        "        soup=bs(requests.get(url).text,'html.parser')\n",
        "        title  = soup.findAll(class_='campaign-title')[0].text\n",
        "        location  = soup.findAll(class_='icon-link location-name js-location-link')[0].text[3:].strip()\n",
        "\n",
        "        try: text = soup.findAll(class_=\"co-story truncate-text truncate-text--description js-truncate\")[0].text.strip()\n",
        "        except IndexError: text = 'exception occured for' + url\n",
        "          \n",
        "        try: likes =  soup.findAll(class_='roundedNum')[0].text\n",
        "        except IndexError: likes = 0\n",
        "          \n",
        "        try: photos = soup.findAll(class_='open-media-viewer')[0].text.strip()\n",
        "        except IndexError: photos = 0\n",
        "\n",
        "        try: shares = soup.findAll(class_='js-share-count-text')[0].text.strip()\n",
        "        except IndexError: shares = 0\n",
        "           \n",
        "        try: start_date = soup.findAll(class_='created-date')[0].text[8:]\n",
        "        except Exception as e:\n",
        "          print('error getting start date:',e)\n",
        "          start_date = e\n",
        "        try: \n",
        "          donation = soup.findAll(class_='campaign-status text-small')[0].text.strip()\n",
        "          recent_donation_time = soup.findAll(class_='supporter-time')[0].text.strip()\n",
        "          donation = donation.split(' ')\n",
        "          donation_count = donation[2]\n",
        "          duration = ' '.join(donation[-2:])\n",
        "          funds  = soup.findAll('h2',class_='goal')[0].text\n",
        "          raised = re.findall('\\$\\d+.*',funds)[0]\n",
        "          goal = re.findall('\\$\\d+.*',funds)[1].split(' ')[0]\n",
        "          \n",
        "          print('\\n',url[25:],raised,'/',goal,'-',donation_count,' ',end='')\n",
        "          if int(re.sub('[^\\d]','',raised)) >= int(re.sub('[^\\d]','',goal)):\n",
        "            min_completion_time = self.get_min_goal_time(url,goal)\n",
        "          else:\n",
        "            min_completion_time = -1\n",
        "        except IndexError:\n",
        "          donation_count = duration = recent_donation_time = raised = goal = min_completion_time = 0\n",
        "          \n",
        "        return OrderedDict({'category':category,'page':page,'title':title, 'href':url, 'location':location,'start_date':start_date ,'goal':goal \n",
        "                            ,'raised':raised, 'text':text ,'likes':likes,'shares':shares, 'photos':photos,  'donation_count':donation_count\n",
        "                            ,'duration':duration, 'recent_donation_time':recent_donation_time , 'goal_reached_time':min_completion_time\n",
        "                            ,'script_run_time':datetime.today().strftime(\"%Y-%m-%d\")})\n",
        "\n",
        "    def get_min_goal_time(self,href,goal):\n",
        "        goal=int(re.sub('[^\\d]','',goal))\n",
        "        campaign = href[25:]\n",
        "        idx = 0\n",
        "        min_completion_time = 0\n",
        "        donation = []\n",
        "        time_gap=[]\n",
        "        while True:\n",
        "            url = 'https://www.gofundme.com/mvc.php?route=donate/pagingDonationsFoundation&url='+campaign+'&idx='+str(idx)+'&type=recent'\n",
        "            soup = requests.get(url)\n",
        "            soup=bs(soup.text,'html.parser')\n",
        "            dn = [i.text for i in soup.findAll(class_='supporter-amount')]\n",
        "            if len(dn)<1:break\n",
        "            donation = donation + dn\n",
        "            time_gap = time_gap+ [i.text[:-4] for i in soup.findAll(class_='supporter-time')]\n",
        "            idx+=10\n",
        "            if idx%100==0:\n",
        "              print('.',end='')\n",
        "        l=[int(re.sub('[^\\d]','',i)) for i in donation[::-1]]\n",
        "        d=list(accumulate(l))\n",
        "        for i in range(len(d)):\n",
        "          if d[i]>goal:\n",
        "            return time_gap[-i-1]\n",
        "\n",
        "    def get_campaigns(self,categories = 'all',skip = ['']):\n",
        "        start_time = time()\n",
        "        df = pd.DataFrame({})\n",
        "        if categories == 'all':\n",
        "          categories = self.get_categories()\n",
        "        campaigns =self.read_data()\n",
        "        print(campaigns.shape)\n",
        "        for i in categories:\n",
        "          if i in skip:\n",
        "            continue\n",
        "          print(i,end='  ')\n",
        "          i='-'.join(i.split(' '))\n",
        "          i = 'animal' if i == 'Animals' else i\n",
        "          url = 'https://www.gofundme.com/discover/'+i+'-fundraiser'\n",
        "          soup = bs(requests.get(url).text,'html.parser')\n",
        "          cid = re.findall('\\d+',re.findall('cid=\\'\\s\\+\\s\\'\\d+', soup.find_all('script')[13].text)[0])[0]\n",
        "          if i in campaigns.category.unique():\n",
        "            page = campaigns.loc[campaigns.category ==i,'page'].max()+1\n",
        "          else:\n",
        "            page = 1\n",
        "            \n",
        "          while True:\n",
        "            print('\\n',page)\n",
        "            url = 'https://www.gofundme.com/mvc.php?route=categorypages/load_more&page='+str(page)+'&term=&cid='+cid\n",
        "            soup = requests.get(url)\n",
        "            soup = bs(soup.text, 'html.parser')\n",
        "            if len(soup) <1: break\n",
        "            details =defaultdict(list)\n",
        "            href = [i['href'] for i in soup.findAll('a',attrs={'class':'campaign-tile-img--contain'})]\n",
        "            for link in href:\n",
        "              for key, value in self.details_parser(link,i,page).items():\n",
        "                details[key].append(value)\n",
        "            if len(details)>0:   \n",
        "              df = pd.DataFrame(details)[self.campaign_columns]\n",
        "              df.to_csv('campaigns.csv',index=False,header=False,mode='a')\n",
        "            if (page%20==0):\n",
        "              #break\n",
        "              self.git_push()\n",
        "            page+=1        \n",
        "                \n",
        "          print('\\n')\n",
        "        #clear_output()\n",
        "        print('campaigns scrape time', time()-start_time)\n",
        "\n",
        "scraper = web_scraper()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zxEQvHRGuXPp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scraper.git_clone()\n",
        "scraper.get_campaigns() \n",
        "#scraper.git_push()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}